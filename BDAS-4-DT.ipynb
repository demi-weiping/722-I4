{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d607f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('DBAS-Step4-Data Transformation').getOrCreate()\n",
    "\n",
    "# Enable pandas-on-Spark\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "print(pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f219f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data from csv file\n",
    "spk_df = spark.read.csv(\"Data/3DP/heart_failure_dataset_3DP.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert Spark DataFrame to pandas-on-Spark DataFrame using to_pandas_on_spark()\n",
    "spkpd_df = spk_df.to_pandas_on_spark()\n",
    "spkpd_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e6943",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# ### ---------- 04-DT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f5aff",
   "metadata": {},
   "source": [
    "#  Balancing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64265f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ---------- 04-DT\n",
    "# Add any transformation steps\n",
    "\n",
    "#from sklearn.utils import resample\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pieplot before boosting\n",
    "death_event_counts = spk_df.groupBy(\"DEATH_EVENT\").count().collect()\n",
    "sizes = [row[\"count\"] for row in death_event_counts]\n",
    "labels = [f\"{status} ({size})\" for status, size in zip([\"Deceased\",\"Alive\"], sizes)]\n",
    "\n",
    "# Use matplotlib \n",
    "colors = ['yellowgreen', 'lightcoral']\n",
    "explode = (0.1, 0)  # explode 1st slice for emphasis\n",
    "\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.title('Distribution of DEATH_EVENT before balancing')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting\n",
    "df_high = spk_df.filter(F.col(\"DEATH_EVENT\") == 0)\n",
    "df_low = spk_df.filter(F.col(\"DEATH_EVENT\") == 1)\n",
    "\n",
    "# boosting with sample function \n",
    "df_low_boost = df_low.sample(True, float(df_high.count()) / df_low.count(), seed=42)\n",
    "spk_df_boosted = df_high.union(df_low_boost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f275a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pieplot after boosting\n",
    "\n",
    "spk_df = spk_df_boosted\n",
    "death_event_counts = spk_df.groupBy(\"DEATH_EVENT\").count().collect()\n",
    "sizes = [row[\"count\"] for row in reversed(death_event_counts)]\n",
    "labels = [f\"{status} ({size})\" for status, size in zip([\"Deceased\", \"Alive\"], sizes)]\n",
    "\n",
    "# Use matplotlib for Pieplot\n",
    "colors = ['yellowgreen','lightcoral']\n",
    "explode = (0.1, 0)  # explode 1st slice for emphasis\n",
    "\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.title('Distribution of DEATH_EVENT After balancing')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045488a6",
   "metadata": {},
   "source": [
    "# Reducing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### -- Reducing data \n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "# Convert 'DEATH_EVENT' from boolean to integer\n",
    "spk_df = spk_df.withColumn(\"DEATH_EVENT\", col(\"DEATH_EVENT\").cast(\"int\"))\n",
    "\n",
    "# Prepare data for MLlib\n",
    "feature_cols = [col for col in spk_df.columns if col not in ['DEATH_EVENT', 'Age_Level']]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(spk_df)\n",
    "\n",
    "# Fit a RandomForest model\n",
    "clf = RandomForestClassifier(numTrees=100, labelCol=\"DEATH_EVENT\", featuresCol=\"features\")\n",
    "model = clf.fit(df_assembled)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.featureImportances.toArray()\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the features based on importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0392006",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select tail 5 features\n",
    "tail_k = 5\n",
    "tail_features = importance_df['Feature'].tail(tail_k).tolist()\n",
    "print(\"tail_features:\")\n",
    "print(\" \",tail_features)\n",
    "\n",
    "# Ignore the 4 features \n",
    "# which are highly related to survival prediction but not our objuectives.\n",
    "\n",
    "ignore_list = ['time','follow_up_month','age','Age_Level']\n",
    "print(\"ignore_list:\")\n",
    "print(\" \", ignore_list)\n",
    "\n",
    "drop_list = tail_features + ignore_list\n",
    "print(\"drop_list:\")\n",
    "print(\" \",drop_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0420b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce data to drop the 9 columes\n",
    "df_reduced = spk_df.drop(*drop_list)\n",
    "df_reduced.show()\n",
    "\n",
    "spk_df = df_reduced\n",
    "spk_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to pandas-on-Spark DataFrame using to_pandas_on_spark()\n",
    "spkpd_df = spk_df.to_pandas_on_spark()\n",
    "spkpd_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV file\n",
    "spk_df.coalesce(1).write.csv(\"Data/4DT\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
